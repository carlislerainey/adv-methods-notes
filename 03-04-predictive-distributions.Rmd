
```{r include=FALSE}
library(tidyverse)
```

## [Posterior] Predictive Distribution

As with simple models without covariates, we can use the predictive distribution and the posterior predictive distribution to understand models *with* covariate. In fact, these tools become more valuable as the complexity of the model increases.

### ... for the logit model

```{r, cache = TRUE}
scobit <- haven::read_dta("data/scobit.dta") %>%
  filter(newvote != -1) %>%  # weird -1s in data; unsure if sufficient
  glimpse()

f <- newvote ~ poly(neweduc, 2, raw = TRUE) + closing + poly(age, 2, raw = TRUE) + south + gov

fit <- glm(f, data = scobit, family = "binomial")

# compute estimates of linear predictor and pi
linpred_hat <- predict(fit, type = "link")  # on scale of linear predictor
pi_hat <- predict(fit, type = "response")   # on probability scale

# put observed data into a data frame with linpred and pi ests
observed_data <- scobit %>%
  mutate(type = "observed", 
         linpred_hat = linpred_hat, 
         pi_hat = pi_hat)

# create data frames with simulated data from predictive distribution
sim_list <- list()
for (i in 1:5) {
  y_tilde <- rbinom(nrow(observed_data), size = 1, prob = pi_hat)
  sim_list[[i]] <- observed_data %>%
    mutate(newvote = y_tilde, 
           type = paste0("simulated #", i))
}

# bind data together
gg_data <- bind_rows(sim_list) %>%
  bind_rows(observed_data) %>%
  glimpse()

# plot fake and obs data against linear predictor.
ggplot(gg_data, aes(x = linpred_hat, y = newvote)) + 
  geom_jitter(height = 0.05, alpha = 0.01, shape = 21, size = 0.3) + 
  facet_wrap(vars(type)) + 
  geom_smooth()

# plot fake and obs data against age.
ggplot(gg_data, aes(x = age, y = newvote)) + 
  geom_jitter(height = 0.05, alpha = 0.01, shape = 21, size = 0.3) + 
  facet_wrap(vars(type)) + 
  geom_smooth(se = FALSE)
```

This model, because we included a second-order polynomial for `age`, does a great job of picking up the nonlinear relationship between age and voting. If we replace the polynomial with a simple linear term, then the observed and predictive distributions show a stark dissimilarity.

```{r}
f <- newvote ~ poly(neweduc, 2, raw = TRUE) + closing + age + south + gov
fit <- glm(f, data = scobit, family = "binomial")

observed_data <- scobit %>%
  mutate(type = "observed", 
         linpred_hat = predict(fit, type = "link"))

sim_list <- list()
for (i in 1:5) {
  y_tilde <- rbinom(nrow(observed_data), size = 1, prob = plogis(observed_data$linpred_hat))
  sim_list[[i]] <- observed_data %>%
    mutate(newvote = y_tilde, 
           type = paste0("simulated #", i))
}
gg_data <- bind_rows(sim_list) %>%
  bind_rows(observed_data) %>%
  glimpse()

ggplot(gg_data, aes(x = age, y = newvote)) + 
  geom_jitter(height = 0.05, alpha = 0.01, shape = 21, size = 0.3) + 
  facet_wrap(vars(type)) + 
  geom_smooth(se = FALSE)
```

We can do this same thing with Stan. However, working with the posterior simulations can be tricky. I use tidybayes `add_predicted_draws()` function along with some clever pivoting to get the data ready for `ggplot()`.

```{r results = "hide", cache = TRUE}
library(rstanarm); options(mc.cores = parallel::detectCores())

small_scobit <- sample_n(scobit, 1000)  # subsample b/c model is slow
stan_fit <- stan_glm(f, data = small_scobit, family = "binomial")
```

```{r}
library(tidybayes)

ppd <- small_scobit %>%
  add_predicted_draws(stan_fit, ndraws = 8) %>%
  mutate(.draw = paste0("Draw #", .draw)) %>% 
  pivot_wider(names_from = .draw, values_from = .prediction) %>%
  mutate(`Observed` = newvote) %>% 
  pivot_longer(`Draw #1`:`Observed`, names_to = "type", values_to = "newvote2") %>%
  glimpse()

ggplot(ppd, aes(x = age, y = newvote2)) + 
  geom_jitter(height = 0.15, alpha = 0.2, shape = 21, size = 0.3) + 
  facet_wrap(vars(type)) + 
  geom_smooth(se = FALSE)
```

### ... for the Poisson model

The code below repeated this exercise for the Poisson model using the HKS data.

```{r}

# load hks data
hks <- read_csv("data/hks.csv") %>%
  na.omit()

# fit poisson model
f <- osvAll ~ troopLag + policeLag + militaryobserversLag + 
  brv_AllLag + osvAllLagDum + incomp + epduration + 
  lntpop
fit <- glm(f, data = hks, family = poisson)

# simulate fake data from predictive distribution
observed_data <- hks %>%
  mutate(type = "observed", 
         linpred_hat = predict(fit, type = "link"))
sim_list <- list()
for (i in 1:5) {
  sim_list[[i]] <- observed_data %>%
    mutate(osvAll = rpois(nrow(observed_data), 
                          lambda = exp(observed_data$linpred_hat)),
           type = paste0("simulated #", i))
}
gg_data <- bind_rows(sim_list) %>%
  bind_rows(observed_data) %>%
  glimpse()

# plot fake and observed data against linear predictor
ggplot(gg_data, aes(x = linpred_hat, y = osvAll + 1)) + 
  geom_point(alpha = 0.1, shape = 21, size = 0.3) + 
  facet_wrap(vars(type)) + 
  scale_y_log10()

# plot fake and observed data against number of troops
ggplot(gg_data, aes(x = troopLag, y = osvAll + 1)) + 
  geom_point(alpha = 0.3, shape = 21, size = 0.3) + 
  facet_wrap(vars(type)) + 
  scale_y_log10() + 
  geom_smooth(se = FALSE)
```

And below is code to work with the posterior predictive distribution.

```{r, results = "hide", cache = TRUE}
stan_fit <- stan_glm(f, data = hks, family = "poisson", chains = 1)
```

```{r}
ppd <- hks %>%
  add_predicted_draws(stan_fit, ndraws = 8) %>% 
  mutate(.draw = paste0("Draw #", .draw)) %>% 
  pivot_wider(names_from = .draw, values_from = .prediction) %>%
  mutate(`Observed` = osvAll) %>% 
  pivot_longer(`Draw #1`:`Observed`, names_to = "type", values_to = "osvAll2") %>%
  glimpse()

ggplot(ppd, aes(x = troopLag, y = osvAll2 + 1)) + 
  geom_point(alpha = 0.2, shape = 21, size = 0.3) + 
  facet_wrap(vars(type)) + 
  geom_smooth(se = FALSE) + 
  scale_y_log10()
```
